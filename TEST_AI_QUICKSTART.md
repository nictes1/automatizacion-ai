# ü§ñ Tests con Cliente AI - Quickstart

Sistema de tests conversacionales que simula usuarios reales con diferentes personalidades.

## ‚ö° Ejecuci√≥n R√°pida

```bash
# M√©todo 1: Script autom√°tico (verifica todo)
./scripts/run_ai_tests.sh

# M√©todo 2: Directo
python3 tests/test_ai_client_scenarios.py
```

## üìã Requisitos Pre-Ejecuci√≥n

### 1. Servicios activos

```bash
# Verificar orchestrator
curl http://localhost:8005/health

# Si no est√° activo:
python3 services/orchestrator_app.py
# O con Docker:
docker-compose up -d orchestrator
```

### 2. Ollama con modelos

```bash
# Verificar modelos
docker exec pulpo-ollama ollama list

# Descargar si faltan:
docker exec pulpo-ollama ollama pull qwen2.5:14b
docker exec pulpo-ollama ollama pull llama3.1:8b
```

## üé≠ Escenarios de Test

### 7 Escenarios Pre-configurados

1. **Cliente Eficiente** (Peluquer√≠a)
   - Da toda la info de golpe
   - Ejemplo: *"Hola, soy Mar√≠a, necesito corte ma√±ana 3pm, maria@gmail.com"*

2. **Cliente Conversacional** (Peluquer√≠a)
   - Info progresiva, natural
   - Usa muletillas: "dale", "perfecto"

3. **Cliente Olvidadizo** (Peluquer√≠a)
   - Responde con contexto extra
   - A veces no da exactamente lo pedido

4. **Cliente Ca√≥tico** (Peluquer√≠a)
   - Info desordenada, texto pegado
   - Ejemplo: *"11am carlos@gmail.com Corte Carlos Mart√≠nez"*

5. **Cliente Breve** (Peluquer√≠a)
   - Respuestas de 1-3 palabras
   - "s√≠", "ok", "ma√±ana 2pm"

6. **Cliente Eficiente** (Restaurante)
   - Reserva de mesa con toda la info

7. **Cliente Conversacional** (Inmobiliaria)
   - Visita a propiedad progresiva

## üìä Salida Esperada

```
================================================================================
üöÄ EJECUTANDO 7 ESCENARIOS DE TEST
================================================================================

üß™ TEST: Cliente Eficiente - Peluquer√≠a
üë§ Usuario: Hola, soy Mar√≠a Gonz√°lez, necesito...
ü§ñ Asistente: ¬°Hola Mar√≠a! Perfecto, te anoto...
‚úÖ TEST EXITOSO (2 turnos, 6.3s)

...

================================================================================
üìä RESUMEN DE RESULTADOS
================================================================================

‚úÖ Exitosos: 7/7 (100.0%)
‚ùå Fallidos: 0/7 (0.0%)

üìà M√âTRICAS GENERALES:
   Promedio turnos: 3.7
   Promedio duraci√≥n: 11.7s
   Total tiempo: 82.0s

üíæ Resultados guardados en: test_results.json
```

## üîç An√°lisis de Resultados

### Ver resultados JSON formateados

```bash
# Resumen
cat test_results.json | jq '.[] | {scenario: .scenario_name, success: .success, turns: .turns}'

# Ver conversaci√≥n espec√≠fica
cat test_results.json | jq '.[0].conversation_log'

# Ver slots extra√≠dos
cat test_results.json | jq '.[] | {scenario: .scenario_name, slots: .slots_collected}'
```

### Analizar fallos

```bash
# Tests fallidos
cat test_results.json | jq '.[] | select(.success == false) | {scenario: .scenario_name, missing: .slots_missing, error: .error}'
```

## ‚öôÔ∏è Configuraci√≥n

### Modificar escenarios

Edita `tests/test_ai_client_scenarios.py`:

```python
# Agregar nuevo escenario
SCENARIOS.append(
    TestScenario(
        id="servicios_custom",
        name="Mi Test Custom",
        description="Descripci√≥n del test",
        vertical="servicios",
        client=ClientProfile(
            name="Juan P√©rez",
            email="juan@example.com",
            phone="+5491123456789",
            personality=PersonalityType.CONVERSATIONAL,
            style_notes="Amigable, usa 'dale'"
        ),
        objective="Agendar turno",
        context={
            "service": "Corte",
            "preferred_date": "2025-10-07",
            "preferred_time": "15:00",
            "when_human": "ma√±ana 3pm"
        },
        expected_slots=["service_type", "preferred_date", "preferred_time", "client_name", "client_email"]
    )
)
```

### Ejecutar solo algunos escenarios

```python
# En test_ai_client_scenarios.py, al final:
if __name__ == "__main__":
    # Solo primeros 3 escenarios
    runner = TestRunner(SCENARIOS[:3])
    asyncio.run(runner.run_all())
```

## üêõ Troubleshooting

### Error: Orchestrator no disponible

```bash
# Verificar
curl http://localhost:8005/health

# Iniciar
python3 services/orchestrator_app.py
```

### Error: Modelo no encontrado

```bash
# Listar modelos
docker exec pulpo-ollama ollama list

# Descargar faltantes
docker exec pulpo-ollama ollama pull qwen2.5:14b
```

### Tests muy lentos

- Qwen2.5:14b requiere ~11GB VRAM
- Si no tienes GPU suficiente, usa modelo m√°s peque√±o:
  ```python
  # En orchestrator_service.py cambiar:
  OLLAMA_MODEL = "qwen2.5:7b-instruct"  # M√°s r√°pido, menos preciso
  ```

### Cliente AI responde mal

- llama3.1:8b es usado para simular cliente
- Si respuestas no naturales, ajustar prompts en `AIClient.generate_response()`

## üìà M√©tricas de √âxito

### Por Test
- ‚úÖ **Success**: Todos los slots recolectados + EXECUTE_ACTION
- üî¢ **Turns**: Menor es mejor (eficiencia)
- ‚è±Ô∏è **Duration**: <15s por test es aceptable

### Suite Completa
- üéØ **Target**: 100% success rate
- ‚ö° **Performance**: <2 minutos para 7 tests
- üìä **Avg Turns**: 3-5 turnos es √≥ptimo

## üìö Documentaci√≥n Completa

- `tests/README_AI_TESTS.md` - Gu√≠a completa
- `tests/test_ai_client_scenarios.py` - C√≥digo fuente
- `SISTEMA_CONVERSACIONAL_FINAL.md` - Sistema conversacional
- `MODEL_COMPARISON.md` - Comparaci√≥n de modelos

## üöÄ Next Steps

Despu√©s de tests exitosos:

1. **Integraci√≥n continua**
   ```bash
   # Agregar a CI/CD pipeline
   ./scripts/run_ai_tests.sh || exit 1
   ```

2. **Expandir escenarios**
   - Casos edge: fechas inv√°lidas, horarios no disponibles
   - M√°s personalidades: enojado, confundido, etc.
   - M√°s verticales: e-commerce, servicios m√©dicos

3. **Validaci√≥n de producci√≥n**
   - Correr tests pre-deploy
   - Monitorear m√©tricas vs producci√≥n real

---

**¬øProblemas?** Revisa `tests/README_AI_TESTS.md` o logs del orchestrator:
```bash
docker logs pulpo-orchestrator -f
```
