# ‚úÖ Checklist de Deployment - SLM Pipeline

Checklist completo para desplegar el SLM Pipeline a producci√≥n con canary deployment.

---

## üìã Pre-Deployment

### 1. C√≥digo y Tests

- [ ] **C√≥digo revisado y mergeado a `main`**
  - [ ] `services/slm/extractor.py`
  - [ ] `services/slm/planner.py`
  - [ ] `services/response/simple_nlg.py`
  - [ ] `services/orchestrator_slm_pipeline.py`
  - [ ] `services/orchestrator_integration.py`
  - [ ] `api/webhook_adapter.py`
  - [ ] `api/routes/webhook_twilio.py`

- [ ] **Tests ejecutados y pasando**
  ```bash
  pytest tests/unit/test_planner_slm.py -v
  pytest tests/e2e/test_slm_pipeline_e2e.py -v
  ./tests/smoke/smoke_test.sh
  ```

- [ ] **Linters y type checking OK**
  ```bash
  ruff check services/
  mypy services/ --ignore-missing-imports
  ```

### 2. Configuraci√≥n

- [ ] **Variables de entorno configuradas**
  - [ ] `ENABLE_SLM_PIPELINE=true`
  - [ ] `SLM_CANARY_PERCENT=10` (empezar con 10%)
  - [ ] `SLM_EXTRACTOR_MODEL=qwen2.5:7b`
  - [ ] `SLM_PLANNER_MODEL=qwen2.5:7b`
  - [ ] `SLM_CONFIDENCE_THRESHOLD=0.7`
  - [ ] `SLM_FALLBACK_TO_LLM=true`
  - [ ] Timeouts configurados (ver `.env.example`)

- [ ] **Schemas JSON presentes**
  - [ ] `config/schemas/extractor_v1.json`
  - [ ] `config/schemas/planner_v1.json`
  - [ ] `config/schemas/response_v1.json`

- [ ] **Modelos SLM disponibles**
  ```bash
  # Ollama
  ollama list | grep qwen2.5:7b
  ollama list | grep phi3:mini
  
  # vLLM (si aplica)
  curl http://localhost:8001/v1/models
  ```

### 3. Infraestructura

- [ ] **Base de datos migrada**
  ```bash
  alembic upgrade head
  ```

- [ ] **Secrets configurados**
  - [ ] API keys (Anthropic, OpenAI, etc.)
  - [ ] Twilio credentials
  - [ ] Database URL

- [ ] **Recursos suficientes**
  - [ ] CPU: 4+ cores
  - [ ] RAM: 16GB+ (para SLMs 7B)
  - [ ] GPU: Opcional pero recomendado (RTX 3090, A100, etc.)
  - [ ] Disk: 50GB+ libre

### 4. Observabilidad

- [ ] **Logging configurado**
  - [ ] Logs estructurados habilitados
  - [ ] Log level: `INFO` en prod, `DEBUG` en staging
  - [ ] Rotaci√≥n de logs configurada

- [ ] **M√©tricas configuradas**
  - [ ] Prometheus exporters activos
  - [ ] Dashboard Grafana creado
  - [ ] Alertas configuradas:
    - [ ] Latencia p90 > 2000ms
    - [ ] Error rate > 2%
    - [ ] Confidence < 0.7 (> 10% requests)

- [ ] **Tracing (opcional)**
  - [ ] OpenTelemetry configurado
  - [ ] Jaeger/Zipkin disponible

### 5. Rollback Plan

- [ ] **Backup de c√≥digo anterior**
  ```bash
  git tag pre-slm-pipeline-$(date +%Y%m%d)
  git push origin --tags
  ```

- [ ] **Rollback script preparado**
  ```bash
  # rollback.sh
  export ENABLE_SLM_PIPELINE=false
  systemctl restart pulpo-api
  ```

- [ ] **Equipo notificado**
  - [ ] Slack/Discord channel activo
  - [ ] On-call engineer asignado
  - [ ] Escalation path definido

---

## üöÄ Deployment

### Fase 1: Staging (D√≠a -1)

- [ ] **Deploy a staging**
  ```bash
  git checkout main
  git pull origin main
  docker-compose -f docker-compose.staging.yml up -d
  ```

- [ ] **Smoke tests en staging**
  ```bash
  API_URL=https://staging.pulpoai.com ./tests/smoke/smoke_test.sh
  ```

- [ ] **Validaci√≥n manual**
  - [ ] Saludo
  - [ ] Consulta horarios
  - [ ] Consulta precios
  - [ ] Reserva completa
  - [ ] Reserva incompleta
  - [ ] Cancelaci√≥n

- [ ] **M√©tricas baseline**
  - [ ] Latencia p50/p90/p99
  - [ ] Error rate
  - [ ] Confidence promedio

### Fase 2: Canary 10% (D√≠a 1-2)

- [ ] **Deploy a producci√≥n con canary 10%**
  ```bash
  export ENABLE_SLM_PIPELINE=true
  export SLM_CANARY_PERCENT=10
  
  # Deploy
  docker-compose -f docker-compose.prod.yml up -d
  ```

- [ ] **Monitoreo cada 2 horas**
  - [ ] Dashboard Grafana abierto
  - [ ] Logs en tiempo real
  - [ ] Alertas configuradas

- [ ] **Validaciones (cada 6 horas)**
  - [ ] Smoke tests
  - [ ] Revisar logs de errores
  - [ ] Comparar m√©tricas SLM vs Legacy

- [ ] **Criterios de √©xito (48 horas)**
  - [ ] ‚úÖ p90 latency < 1500ms
  - [ ] ‚úÖ Error rate < 1%
  - [ ] ‚úÖ No quejas de usuarios
  - [ ] ‚úÖ Confidence promedio > 0.8

### Fase 3: Canary 50% (D√≠a 3-4)

- [ ] **Elevar a 50%**
  ```bash
  export SLM_CANARY_PERCENT=50
  systemctl restart pulpo-api
  ```

- [ ] **An√°lisis comparativo**
  - [ ] Latencia: SLM vs Legacy
  - [ ] Accuracy: SLM vs Legacy
  - [ ] User satisfaction: Feedback/quejas

- [ ] **Identificar edge cases**
  - [ ] Revisar requests con confidence < 0.7
  - [ ] Revisar policy denials
  - [ ] Revisar tool failures

- [ ] **Ajustes (si es necesario)**
  - [ ] Prompts de Extractor
  - [ ] Prompts de Planner
  - [ ] Timeouts
  - [ ] Thresholds

### Fase 4: Full SLM (D√≠a 5+)

- [ ] **Elevar a 100%**
  ```bash
  export SLM_CANARY_PERCENT=0  # 0 = 100% SLM
  systemctl restart pulpo-api
  ```

- [ ] **Monitoreo intensivo (48 horas)**
  - [ ] Dashboard abierto 24/7
  - [ ] On-call engineer disponible
  - [ ] Rollback plan listo

- [ ] **Validaci√≥n final**
  - [ ] Smoke tests cada 12 horas
  - [ ] Revisar m√©tricas diarias
  - [ ] Feedback de usuarios

---

## üìä Post-Deployment

### Semana 1: Observaci√≥n

- [ ] **M√©tricas diarias**
  - [ ] Latencia p50/p90/p99
  - [ ] Error rate
  - [ ] Confidence distribution
  - [ ] Tool success rate
  - [ ] Policy denial rate

- [ ] **An√°lisis de logs**
  - [ ] Top 10 errores
  - [ ] Requests m√°s lentos
  - [ ] Intents m√°s comunes
  - [ ] Tools m√°s usados

- [ ] **Feedback de usuarios**
  - [ ] Revisar quejas/elogios
  - [ ] Encuestas de satisfacci√≥n
  - [ ] NPS score

### Semana 2: Optimizaci√≥n

- [ ] **Fine-tuning de prompts**
  - [ ] Extractor: Mejorar few-shot examples
  - [ ] Planner: Ajustar reglas de decisi√≥n
  - [ ] Response: Mejorar plantillas

- [ ] **Optimizaci√≥n de latencia**
  - [ ] Identificar bottlenecks
  - [ ] Paralelizar tools cuando sea posible
  - [ ] Cachear respuestas frecuentes

- [ ] **Golden dataset**
  - [ ] Crear 10 conversaciones por intent
  - [ ] Automatizar tests en CI/CD
  - [ ] Baseline de m√©tricas

### Mes 1: Consolidaci√≥n

- [ ] **Documentaci√≥n actualizada**
  - [ ] README principal
  - [ ] Runbook de troubleshooting
  - [ ] Decisiones arquitect√≥nicas

- [ ] **Training del equipo**
  - [ ] Sesi√≥n de onboarding
  - [ ] Demo del pipeline
  - [ ] Q&A session

- [ ] **Planificaci√≥n siguiente fase**
  - [ ] LLM fallback autom√°tico
  - [ ] Fine-tuning PEFT
  - [ ] Critic SLM
  - [ ] Multi-vertical support

---

## üîß Troubleshooting

### Rollback Inmediato

Si algo sale mal:

```bash
# Opci√≥n 1: Deshabilitar SLM
export ENABLE_SLM_PIPELINE=false
systemctl restart pulpo-api

# Opci√≥n 2: Canary 100% (todo a legacy)
export SLM_CANARY_PERCENT=100
systemctl restart pulpo-api

# Opci√≥n 3: Rollback a versi√≥n anterior
git checkout pre-slm-pipeline-YYYYMMDD
docker-compose -f docker-compose.prod.yml up -d --build
```

### Contactos de Emergencia

- **Tech Lead**: [nombre] - [email] - [phone]
- **DevOps**: [nombre] - [email] - [phone]
- **On-Call**: [rotation] - [slack channel]

---

## üìù Sign-off

### Aprobaciones Requeridas

- [ ] **Tech Lead**: _________________ Fecha: _______
- [ ] **Product Manager**: _________________ Fecha: _______
- [ ] **DevOps**: _________________ Fecha: _______

### Post-Deployment Review

- [ ] **Retrospectiva agendada** (1 semana post-deploy)
- [ ] **M√©tricas documentadas** (baseline vs post-deploy)
- [ ] **Lecciones aprendidas** documentadas

---

**√öltima actualizaci√≥n:** 15 Enero 2025  
**Versi√≥n:** 1.0  
**Estado:** ‚úÖ READY FOR DEPLOYMENT




