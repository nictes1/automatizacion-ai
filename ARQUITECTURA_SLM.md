# üß† Arquitectura SLM-First para PulpoAI

## üéØ Principios

1. **SLM-first**: Modelos 3B-8B para casi todo (barato/r√°pido)
2. **Contratos JSON**: Schemas versionados, cero texto libre entre m√≥dulos
3. **Determinismo en el core**: FSM/Policy/ToolBroker hacen la l√≥gica
4. **LLM fallback**: Modelo grande solo si SLM confidence < 0.7
5. **Observabilidad**: Log de I/O por etapa + m√©tricas

---

## üìä Pipeline por Etapa

```
Usuario: "Quiero corte ma√±ana a las 3pm"
    ‚Üì
1. EXTRACTOR SLM (150-250ms)
   Schema: extractor_v1.json
   Output: {intent: "book", slots: {service_type: "Corte", date: "2025-10-16", time: "15:00"}, confidence: 0.92}
    ‚Üì
2. PLANNER SLM (120-200ms)
   Schema: planner_v1.json
   Output: {actions: [{tool: "check_service_availability", args: {...}}], needs_confirmation: false}
    ‚Üì
3. POLICY (determin√≠stico, <10ms)
   Valida: permisos, rate limits, args completos
   Output: actions validadas o error
    ‚Üì
4. TOOL BROKER (variable, 100-500ms)
   Ejecuta tools con retry/circuit breaker
   Output: ToolObservation[]
    ‚Üì
5. FSM + STATE REDUCER (determin√≠stico, <20ms)
   Actualiza estado, decide transici√≥n
   Output: nuevo estado + slots actualizados
    ‚Üì
6. RESPONSE GENERATOR (80-150ms)
   Schema: response_v1.json
   Plantilla determin√≠stica O SLM corto
   Output: {message: "...", tone: "friendly", next_state: "CONFIRMACION"}
```

**Presupuesto total:** 450-1130ms (objetivo p50: <800ms, p90: <1500ms)

---

## üìÅ Schemas JSON (Versionados)

### 1. Extractor v1 (`config/schemas/extractor_v1.json`)

```json
{
  "intent": "book|info_services|info_prices|...",
  "slots": {
    "service_type": "Corte de Cabello|null",
    "preferred_date": "2025-10-16|null",
    "preferred_time": "15:00|null",
    ...
  },
  "confidence": 0.0-1.0
}
```

**Responsabilidad:** Clasificar intent + extraer entidades  
**Modelo:** Qwen2.5-7B Instruct o Phi-3-mini  
**Latencia:** 150-250ms  
**Fallback:** Si confidence < 0.7 ‚Üí LLM mayor

### 2. Planner v1 (`config/schemas/planner_v1.json`)

```json
{
  "plan_version": "v1",
  "actions": [
    {"tool": "get_available_services", "args": {"q": "corte"}},
    {"tool": "check_service_availability", "args": {...}}
  ],
  "needs_confirmation": false,
  "missing_slots": []
}
```

**Responsabilidad:** Decidir qu√© tools ejecutar (NO genera texto)  
**Modelo:** Qwen2.5-7B Instruct con few-shot  
**Latencia:** 120-200ms  
**Fallback:** Si plan inv√°lido ‚Üí Policy rechaza y pide aclaraci√≥n

### 3. Response v1 (`config/schemas/response_v1.json`)

```json
{
  "message": "Perfecto, corte ma√±ana 16/10 a las 15:00. ¬øTu nombre y email?",
  "tone": "friendly",
  "next_state": "CONFIRMACION",
  "quick_replies": ["S√≠", "Cambiar hora"]
}
```

**Responsabilidad:** Generar respuesta final  
**Modelo:** Plantilla determin√≠stica (90%) + Phi-3-mini para re-frasear (10%)  
**Latencia:** 80-150ms  
**Fallback:** Siempre usa plantilla si SLM falla

---

## üîß Componentes Implementados

### ‚úÖ 1. Extractor SLM (`services/slm/extractor.py`)

**Caracter√≠sticas:**
- Constrained decoding con JSON Schema
- Normalizaci√≥n de fechas/horas
- Few-shot prompting con ejemplos del vertical
- Fallback heur√≠stico si SLM falla

**Uso:**
```python
from services.slm.extractor import ExtractorSLM

extractor = ExtractorSLM(llm_client)
result = await extractor.extract("Quiero corte ma√±ana a las 3pm")

print(result.intent)      # "book"
print(result.slots)       # {"service_type": "Corte", "preferred_date": "2025-10-16", ...}
print(result.confidence)  # 0.92
```

### üöß 2. Planner SLM (`services/slm/planner.py`) - TODO

**Caracter√≠sticas:**
- Decide tools bas√°ndose en intent + slots + estado
- Few-shot con ejemplos por vertical
- M√°ximo 3 tools por plan
- Detecta slots faltantes

### üöß 3. Response Generator (`services/slm/response_generator.py`) - TODO

**Caracter√≠sticas:**
- Plantillas determin√≠sticas para casos comunes
- SLM corto solo para casos complejos
- M√°ximo 480 caracteres (2-3 oraciones)
- Quick replies sugeridas

---

## üéõÔ∏è Configuraci√≥n de Modelos

### Opci√≥n 1: Un solo checkpoint, m√∫ltiples prompts
```yaml
model:
  checkpoint: "Qwen/Qwen2.5-7B-Instruct"
  device: "cuda"
  max_batch_size: 4
  
stages:
  extractor:
    temperature: 0.1
    max_tokens: 300
    
  planner:
    temperature: 0.2
    max_tokens: 400
    
  response:
    temperature: 0.7
    max_tokens: 200
```

### Opci√≥n 2: Modelos especializados
```yaml
models:
  extractor:
    checkpoint: "Qwen/Qwen2.5-7B-Instruct"
    temperature: 0.1
    
  planner:
    checkpoint: "Qwen/Qwen2.5-7B-Instruct"
    temperature: 0.2
    
  response:
    checkpoint: "microsoft/Phi-3-mini-4k-instruct"
    temperature: 0.7
    
  fallback:
    checkpoint: "Qwen/Qwen2.5-14B-Instruct"
    temperature: 0.3
```

---

## üìà M√©tricas por Etapa

### Extractor
- **Intent Accuracy**: % de intents correctos (objetivo: >92%)
- **Slot F1**: Precision/Recall de slots extra√≠dos (objetivo: >85%)
- **Confidence Distribution**: Histograma de confidence scores
- **Fallback Rate**: % que usa fallback (objetivo: <5%)
- **Latency p50/p90**: 150ms / 250ms

### Planner
- **Plan Validity**: % de planes que pasan Policy (objetivo: >95%)
- **Tool Accuracy**: % de tools correctos para el intent (objetivo: >90%)
- **Needs Confirmation Rate**: % que requiere aclaraci√≥n (objetivo: 20-30%)
- **Latency p50/p90**: 120ms / 200ms

### Response Generator
- **Template Usage**: % que usa plantillas vs SLM (objetivo: >90% plantillas)
- **Message Length**: Promedio de caracteres (objetivo: <300)
- **Tone Consistency**: % que respeta el tone (objetivo: >95%)
- **Latency p50/p90**: 80ms / 150ms

### End-to-End
- **Total Latency p50/p90**: <800ms / <1500ms
- **Booking Success Rate**: % de bookings completados (objetivo: >65%)
- **Messages per Booking**: Promedio de mensajes (objetivo: <5)
- **Error Rate**: % de errores (objetivo: <1%)

---

## üß™ Golden Tests por Intent

### Greeting
```
Input: "Hola, buenos d√≠as"
Expected: {intent: "greeting", confidence: >0.9}
```

### Info Services
```
Input: "¬øQu√© servicios tienen?"
Expected: {intent: "info_services", confidence: >0.9}
```

### Book (completo)
```
Input: "Quiero corte ma√±ana a las 3pm"
Expected: {
  intent: "book",
  slots: {
    service_type: "Corte de Cabello",
    preferred_date: "2025-10-16",
    preferred_time: "15:00"
  },
  confidence: >0.9
}
```

### Book (incompleto)
```
Input: "Necesito un turno"
Expected: {
  intent: "book",
  slots: {},
  confidence: >0.8
}
```

### Cancel
```
Input: "Quiero cancelar mi turno"
Expected: {intent: "cancel", confidence: >0.85}
```

---

## üöÄ Plan de Implementaci√≥n

### ‚úÖ Semana 1 - Fundamentos
- [x] Schemas JSON versionados
- [x] Extractor SLM con constrained decoding
- [ ] Planner SLM con few-shot
- [ ] Integraci√≥n con orchestrator existente

### üöß Semana 2 - Optimizaci√≥n
- [ ] Response Generator (plantillas + SLM)
- [ ] LLM fallback autom√°tico
- [ ] Dashboard de m√©tricas por etapa
- [ ] Golden tests + CI/CD

### üìã Semana 3 - Refinamiento
- [ ] Fine-tuning PEFT por etapa
- [ ] Shadow mode (SLM vs LLM)
- [ ] Optimizaci√≥n de latencia
- [ ] Documentaci√≥n completa

---

## üí° Ventajas de esta Arquitectura

1. **Control total**: FSM + Policy deciden la l√≥gica, no el LLM
2. **Costos predecibles**: SLMs 10-50x m√°s baratos que GPT-4
3. **Latencia estable**: Presupuestos por etapa, sin sorpresas
4. **Evoluci√≥n gradual**: Fine-tune una etapa sin afectar otras
5. **Debugging f√°cil**: Contratos JSON claros entre m√≥dulos
6. **Multi-tenant seguro**: Validaci√≥n en cada etapa

---

**√öltima actualizaci√≥n:** 15 Enero 2025  
**Versi√≥n:** 1.0





